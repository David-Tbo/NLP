{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Fundamentals of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Topic Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing  \n",
    "\n",
    "Author: D.Thébault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References used:  \n",
    "- https://nlpdemystified.org\n",
    "\n",
    "- https://www.khoury.northeastern.edu/home/vip/teach/DMcourse/5_topicmodel_summ/Lad-Gibbs-lecture032321.pdf\n",
    "\n",
    "- https://arxiv.org/pdf/1505.02065v4\n",
    "\n",
    "- https://www.youtube.com/watch?v=4DsgCZ3KTNk&t=1336s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook we will code the Collapsed Gibbs Sampling for LDA model.**  \n",
    "We will start by an introduction about CGS for LDA.  \n",
    "Then we will import the packages, load the corpus and preprocess it.  \n",
    "Finally, we will code the CGS for LDA step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colapsed Gibbs Sampling (CGS) for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we go from data to a set of topics describing that data ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferring these distributions directly is intractable (prohibitive amount of computational resources).  \n",
    "Instead we use an approximate inference technique called <u>**Collapsed Gibbs Sampling (CGS)**</u>.  \n",
    "\n",
    "CGS simplifies the inference because we don't have to sample explicitly the topics and words distributions.  \n",
    "\n",
    "Instead, CGS estimates these distributions by sampling the topics associated with the words in the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte Carlo Markov Chain (MCMC)**  \n",
    "\n",
    "MCMC is a classical algorithm for sampling from multivariate probability distributions.  \n",
    "It provides an easy way to sample from high dimensional distributions.  \n",
    "\n",
    "For a joint distibution that is difficult to sample from directly, Gibbs sampling  \n",
    "is a way to sample only one variable at a time while fixing the values of the others variables.  \n",
    "In the CGS, the conditional probability which is maximized for topic modelling (LDA)  \n",
    "is the probability to assign a topic $z$ to a word $w$ in a document $d$.  \n",
    "For each word $w$ in a document $d$, the algorithm maximizes the conditional probability:  \n",
    "\n",
    "$p(z \\mid w, d, z_{-i})$  \n",
    "\n",
    "where $z_{-i}$ represents the assignments of topics for all words except $i$.  \n",
    "The word $i$ is temporarly deleted of counting to avoid a bias in the sample.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CGS algorithm  \n",
    "\n",
    "- Part 1. Initialization of the matrices,  \n",
    "\n",
    "- Part 2. Update the topic assignments for each word in each document.  \n",
    "\n",
    "    Loop:  \n",
    "    For each word in each document, we will:  \n",
    "        - 1. Remove the current topic assignment.  \n",
    "        - 2. Compute the probability of assigning the word to each topic.  \n",
    "        - 3. Sample a new topic assignment based on the computed probabilities.  \n",
    "        - 4. Update the count matrices with the new topic assignment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability to assign a word $w_n$ to a topic $k$ in a document $d$ is given by:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(z_{dn}=k \\mid w, \\alpha, \\beta) \n",
    "\\propto \n",
    "\\dfrac{n_{dk}^{-dn} + \\alpha_k}{\\underset{k'}{\\sum}(n_{dk'}^{-dn} + \\alpha_{k'})} \n",
    "\\times\n",
    "\\dfrac{n_{kw_{dn}}^{-dn} + \\beta_{w_{dn}}}{\\underset{w'}{\\sum}(n_{kw'}^{-dn} + \\beta_{w'})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formula combines the probability of assigning a topic $k$ to a document $d$  \n",
    "and the probability of assigning a word $w+{dn}$ to a topic $k$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 0: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "# configuration of the numpy arrays to avoid scientific outputs\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# reproductibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Any lunar satellite needs fuel to do regular orbit corrections, and when\n",
      "its fuel runs out it will crash within months.  The orbits of the Apollo\n",
      "motherships changed noticeably during lunar missions lasting only a few\n",
      "days.  It is *possible* that there are stable orbits here and there --\n",
      "the Moon's gravitational field is poorly mapped -- but we know of none.\n",
      "\n",
      "Perturbations from Sun and Earth are relatively minor issues at low\n",
      "altitudes.  The big problem is that the Moon's own gravitational field\n",
      "is quite lumpy due to the irregular distribution of mass within the Moon.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Glad to see Griffin is spending his time on engineering rather than on\n",
      "ritual purification of the language.  Pity he got stuck with the turkey\n",
      "rather than one of the sensible options.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In spite of my great respect for the people you speak of, I think their\n",
      "cost estimates are a bit over-optimistic. If nothing else, a working SSTO\n",
      "is at least as complex as a large airliner and has a smaller experience\n",
      "base. It therefore seems that SSTO development should cost at least as\n",
      "much as a typical airliner development. That puts it in the $3G to $5G\n",
      "range.\n",
      "\n",
      "\n",
      "True it and the contest would result in a much larger market. But I\n",
      "don't think it would be enough to attract the investors given the\n",
      "risks involved.\n",
      "\n",
      "If you could gurantee the SSTO costs and gurantee that it captures\n",
      "100% of the available launch market, then I think you could\n",
      "do it.\n",
      "\n",
      "  Allen\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Number of words of the first document: 97\n"
     ]
    }
   ],
   "source": [
    "corpus = fetch_20newsgroups(categories=['sci.space'],remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Select the first documents\n",
    "D = 3\n",
    "first_documents = corpus.data[:D]\n",
    "\n",
    "# Print the first documents\n",
    "for doc in first_documents:\n",
    "    print(doc)\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")  # Separator for better readability\n",
    "\n",
    "# Split the text into words and count the number of words\n",
    "words = first_documents[0].split()\n",
    "print(f\"\\n Number of words of the first document: {len(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### corpus -> processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lunar', 'satellite', 'need', 'fuel', 'regular', 'orbit', 'correction', 'fuel', 'run', 'crash', 'month', 'orbit', 'Apollo', 'mothership', 'change', 'noticeably', 'lunar', 'mission', 'last', 'day', 'possible', 'stable', 'orbit', 'Moon', 'gravitational', 'field', 'poorly', 'map', 'know', 'perturbation', 'Sun', 'Earth', 'relatively', 'minor', 'issue', 'low', 'altitude', 'big', 'problem', 'Moon', 'gravitational', 'field', 'lumpy', 'irregular', 'distribution', 'mass', 'Moon']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "['Glad', 'Griffin', 'spend', 'time', 'engineer', 'ritual', 'purification', 'language', 'Pity', 'get', 'stick', 'turkey', 'sensible', 'option']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "['spite', 'great', 'respect', 'people', 'speak', 'think', 'cost', 'estimate', 'bit', 'optimistic', 'work', 'SSTO', 'complex', 'large', 'airliner', 'small', 'experience', 'base', 'SSTO', 'development', 'cost', 'typical', 'airliner', 'development', 'put', 'g', 'g', 'range', 'true', 'contest', 'result', 'large', 'market', 'think', 'attract', 'investor', 'give', 'risk', 'involve', 'gurantee', 'SSTO', 'cost', 'gurantee', 'capture', 'available', 'launch', 'market', 'think', 'Allen']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "CPU times: user 220 ms, sys: 8.94 ms, total: 229 ms\n",
      "Wall time: 231 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load the SpaCy english language model \"en_core_web_sm\"\n",
    "# This model inclused functionalities such as tokenisation, \n",
    "# POS tagging (étiquetage des parties du discours), la reconnaissance des entités nommées (NER), \n",
    "# and syntactic analysis (parsing).\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# We don't need named-entity recognition nor dependency parsing in the pipeline. \n",
    "# We do need part-of-speech tagging however.\n",
    "unwanted_pipes = [\"ner\", \"parser\"]\n",
    "\n",
    "# Preprocessing\n",
    "# For this exercise, we'll remove punctuation, spaces (which\n",
    "# includes newlines) and stopwords filter for tokens consisting of alphabetic\n",
    "# characters, and return the lemma (which require POS tagging).\n",
    "def spacy_tokenizer(doc):\n",
    "  with nlp.disable_pipes(*unwanted_pipes):\n",
    "    return [t.lemma_ for t in nlp(doc) if \\\n",
    "            not t.is_punct and \\\n",
    "            not t.is_space and \\\n",
    "            not t.is_stop and \\\n",
    "            t.is_alpha]  # Check if the token is not a named entity (we want to remove Names)\n",
    "\n",
    "# Apply the preprocessing function to the corpus\n",
    "processed_corpus = []\n",
    "for text in first_documents:\n",
    "    processed_text = spacy_tokenizer(text)\n",
    "    processed_corpus.append(processed_text)\n",
    "\n",
    "# Print the first document in the processed corpus to verify\n",
    "for doc in processed_corpus:\n",
    "    print(doc)\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")  # Separator for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of the parameters\n",
    "\n",
    "We have to initialize the LDA parameters.  \n",
    "The matrix of counting the words and the topics.  \n",
    "\n",
    "##### Parameters of the LDA  \n",
    "\n",
    "- $K$: Number of topics  \n",
    "\n",
    "- $\\alpha$: Parameter of the Dirichlet distribution for the topics of the documents\n",
    "\n",
    "- $\\beta$: Parameter of the Dirichlet distribution for the words in the subjects.  \n",
    "\n",
    "- NUM_ITER: Number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_TOPICS =  3\n",
      "num_docs =  3\n",
      "topic list =  [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "K = 3\n",
    "ALPHA = 0.5\n",
    "BETA = 0.5\n",
    "NUM_ITER = 20\n",
    "\n",
    "num_docs = len(processed_corpus)\n",
    "topic_list = list(range(K))\n",
    "\n",
    "print(\"NUM_TOPICS = \", K)\n",
    "print(\"num_docs = \", num_docs)\n",
    "print(\"topic list = \", topic_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Random assignation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly assign a topic to every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('lunar', 2),\n",
       "  ('satellite', 0),\n",
       "  ('need', 2),\n",
       "  ('fuel', 2),\n",
       "  ('regular', 0),\n",
       "  ('orbit', 0),\n",
       "  ('correction', 2),\n",
       "  ('fuel', 1),\n",
       "  ('run', 2),\n",
       "  ('crash', 2),\n",
       "  ('month', 2),\n",
       "  ('orbit', 2),\n",
       "  ('Apollo', 0),\n",
       "  ('mothership', 2),\n",
       "  ('change', 1),\n",
       "  ('noticeably', 0),\n",
       "  ('lunar', 1),\n",
       "  ('mission', 1),\n",
       "  ('last', 1),\n",
       "  ('day', 1),\n",
       "  ('possible', 0),\n",
       "  ('stable', 0),\n",
       "  ('orbit', 1),\n",
       "  ('Moon', 1),\n",
       "  ('gravitational', 0),\n",
       "  ('field', 0),\n",
       "  ('poorly', 0),\n",
       "  ('map', 2),\n",
       "  ('know', 2),\n",
       "  ('perturbation', 2),\n",
       "  ('Sun', 1),\n",
       "  ('Earth', 2),\n",
       "  ('relatively', 1),\n",
       "  ('minor', 1),\n",
       "  ('issue', 2),\n",
       "  ('low', 1),\n",
       "  ('altitude', 2),\n",
       "  ('big', 2),\n",
       "  ('problem', 0),\n",
       "  ('Moon', 2),\n",
       "  ('gravitational', 0),\n",
       "  ('field', 2),\n",
       "  ('lumpy', 2),\n",
       "  ('irregular', 0),\n",
       "  ('distribution', 0),\n",
       "  ('mass', 2),\n",
       "  ('Moon', 1)],\n",
       " [('Glad', 0),\n",
       "  ('Griffin', 1),\n",
       "  ('spend', 1),\n",
       "  ('time', 1),\n",
       "  ('engineer', 0),\n",
       "  ('ritual', 1),\n",
       "  ('purification', 0),\n",
       "  ('language', 1),\n",
       "  ('Pity', 2),\n",
       "  ('get', 2),\n",
       "  ('stick', 0),\n",
       "  ('turkey', 2),\n",
       "  ('sensible', 2),\n",
       "  ('option', 1)],\n",
       " [('spite', 0),\n",
       "  ('great', 1),\n",
       "  ('respect', 1),\n",
       "  ('people', 1),\n",
       "  ('speak', 1),\n",
       "  ('think', 1),\n",
       "  ('cost', 1),\n",
       "  ('estimate', 1),\n",
       "  ('bit', 0),\n",
       "  ('optimistic', 2),\n",
       "  ('work', 1),\n",
       "  ('SSTO', 1),\n",
       "  ('complex', 1),\n",
       "  ('large', 1),\n",
       "  ('airliner', 1),\n",
       "  ('small', 1),\n",
       "  ('experience', 2),\n",
       "  ('base', 2),\n",
       "  ('SSTO', 1),\n",
       "  ('development', 2),\n",
       "  ('cost', 0),\n",
       "  ('typical', 1),\n",
       "  ('airliner', 0),\n",
       "  ('development', 0),\n",
       "  ('put', 1),\n",
       "  ('g', 2),\n",
       "  ('g', 0),\n",
       "  ('range', 1),\n",
       "  ('true', 0),\n",
       "  ('contest', 0),\n",
       "  ('result', 0),\n",
       "  ('large', 0),\n",
       "  ('market', 2),\n",
       "  ('think', 0),\n",
       "  ('attract', 0),\n",
       "  ('investor', 0),\n",
       "  ('give', 2),\n",
       "  ('risk', 0),\n",
       "  ('involve', 0),\n",
       "  ('gurantee', 2),\n",
       "  ('SSTO', 2),\n",
       "  ('cost', 2),\n",
       "  ('gurantee', 0),\n",
       "  ('capture', 2),\n",
       "  ('available', 2),\n",
       "  ('launch', 0),\n",
       "  ('market', 2),\n",
       "  ('think', 0),\n",
       "  ('Allen', 1)]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# STEP 1: Randomly assign a topic number to every word in every document.\n",
    "# ======\n",
    "z = []\n",
    "for _, doc in enumerate(processed_corpus):\n",
    "    doc_with_topics = [(word, np.random.randint(low=0, high=K)) for word in doc]\n",
    "    z.append(doc_with_topics)\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- $n_{dk}$ : number of words in the document $d$ assigned to the topic $k$ $\\iff$ number of times topic $k$ appears in a document $d$   \n",
    "\n",
    "- $n_{w,k}$ : number of times where the word $w$ is assigned to the topic $k$  \n",
    "\n",
    "- $n_k$ : number of words assigned to the topic $k$  \n",
    "\n",
    "- $n_d$ : number of words in the document $d \\; :$ $\\sum_{i=1}^K{n_{d,i}}$  \n",
    "\n",
    "- $n$ : number of words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([defaultdict(int, {2: 20, 0: 14, 1: 13}),\n",
       "  defaultdict(int, {0: 4, 1: 6, 2: 4}),\n",
       "  defaultdict(int, {0: 18, 1: 18, 2: 13})],\n",
       " {2: defaultdict(int,\n",
       "              {'lunar': 1,\n",
       "               'need': 1,\n",
       "               'fuel': 1,\n",
       "               'correction': 1,\n",
       "               'run': 1,\n",
       "               'crash': 1,\n",
       "               'month': 1,\n",
       "               'orbit': 1,\n",
       "               'mothership': 1,\n",
       "               'map': 1,\n",
       "               'know': 1,\n",
       "               'perturbation': 1,\n",
       "               'Earth': 1,\n",
       "               'issue': 1,\n",
       "               'altitude': 1,\n",
       "               'big': 1,\n",
       "               'Moon': 1,\n",
       "               'field': 1,\n",
       "               'lumpy': 1,\n",
       "               'mass': 1,\n",
       "               'Pity': 1,\n",
       "               'get': 1,\n",
       "               'turkey': 1,\n",
       "               'sensible': 1,\n",
       "               'optimistic': 1,\n",
       "               'experience': 1,\n",
       "               'base': 1,\n",
       "               'development': 1,\n",
       "               'g': 1,\n",
       "               'market': 2,\n",
       "               'give': 1,\n",
       "               'gurantee': 1,\n",
       "               'SSTO': 1,\n",
       "               'cost': 1,\n",
       "               'capture': 1,\n",
       "               'available': 1}),\n",
       "  0: defaultdict(int,\n",
       "              {'satellite': 1,\n",
       "               'regular': 1,\n",
       "               'orbit': 1,\n",
       "               'Apollo': 1,\n",
       "               'noticeably': 1,\n",
       "               'possible': 1,\n",
       "               'stable': 1,\n",
       "               'gravitational': 2,\n",
       "               'field': 1,\n",
       "               'poorly': 1,\n",
       "               'problem': 1,\n",
       "               'irregular': 1,\n",
       "               'distribution': 1,\n",
       "               'Glad': 1,\n",
       "               'engineer': 1,\n",
       "               'purification': 1,\n",
       "               'stick': 1,\n",
       "               'spite': 1,\n",
       "               'bit': 1,\n",
       "               'cost': 1,\n",
       "               'airliner': 1,\n",
       "               'development': 1,\n",
       "               'g': 1,\n",
       "               'true': 1,\n",
       "               'contest': 1,\n",
       "               'result': 1,\n",
       "               'large': 1,\n",
       "               'think': 2,\n",
       "               'attract': 1,\n",
       "               'investor': 1,\n",
       "               'risk': 1,\n",
       "               'involve': 1,\n",
       "               'gurantee': 1,\n",
       "               'launch': 1}),\n",
       "  1: defaultdict(int,\n",
       "              {'fuel': 1,\n",
       "               'change': 1,\n",
       "               'lunar': 1,\n",
       "               'mission': 1,\n",
       "               'last': 1,\n",
       "               'day': 1,\n",
       "               'orbit': 1,\n",
       "               'Moon': 2,\n",
       "               'Sun': 1,\n",
       "               'relatively': 1,\n",
       "               'minor': 1,\n",
       "               'low': 1,\n",
       "               'Griffin': 1,\n",
       "               'spend': 1,\n",
       "               'time': 1,\n",
       "               'ritual': 1,\n",
       "               'language': 1,\n",
       "               'option': 1,\n",
       "               'great': 1,\n",
       "               'respect': 1,\n",
       "               'people': 1,\n",
       "               'speak': 1,\n",
       "               'think': 1,\n",
       "               'cost': 1,\n",
       "               'estimate': 1,\n",
       "               'work': 1,\n",
       "               'SSTO': 2,\n",
       "               'complex': 1,\n",
       "               'large': 1,\n",
       "               'airliner': 1,\n",
       "               'small': 1,\n",
       "               'typical': 1,\n",
       "               'put': 1,\n",
       "               'range': 1,\n",
       "               'Allen': 1})},\n",
       " defaultdict(int, {2: 37, 0: 36, 1: 37}),\n",
       " defaultdict(int, {0: 47, 1: 14, 2: 49}),\n",
       " 110)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 2:\n",
    "\n",
    "# Number of words per topic per document\n",
    "ndk = []\n",
    "for _, doc_with_topic in enumerate(z):\n",
    "    topic_counts = defaultdict(int)\n",
    "    for word, topic in doc_with_topic:\n",
    "        topic_counts[topic] += 1\n",
    "    ndk.append(topic_counts)\n",
    "\n",
    "# Number of times a word is assigned to a topic k across the corpus.\n",
    "nwk = defaultdict(lambda: defaultdict(int))\n",
    "for _, doc_with_topic in enumerate(z):\n",
    "    for word, topic in doc_with_topic:\n",
    "        nwk[topic][word] += 1\n",
    "nwk = dict(nwk)\n",
    "\n",
    "# Number of words per topics across the corpus\n",
    "nk = defaultdict(int)\n",
    "for dico in ndk:\n",
    "    for key, value in dico.items():\n",
    "        nk[key] += value\n",
    "\n",
    "# Number of words per document\n",
    "nd = defaultdict(int)\n",
    "for i, dico in enumerate(ndk):\n",
    "    nd[i] = sum(dico.values())\n",
    "\n",
    "# Number of words of the corpus:\n",
    "n=0\n",
    "for doc in processed_corpus:\n",
    "    n+=len(doc)\n",
    "\n",
    "ndk, nwk, nk, nd, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Unassign / Reassign topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In a document d, unassign a word from its topic  \n",
    "\n",
    "#### Assign w a new topic based on:  \n",
    "- a. How much this document d likes topic k  \n",
    "- b. How much this topic likes word w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31451612903225806, 0.007575757575757576, 0.5030487804878049]\n",
      "[0.10483870967741936, 0.0025252525252525255, 0.1676829268292683]\n",
      "[0.10483870967741936, 0.0025252525252525255, 0.1676829268292683]\n",
      "[0.3271276595744681, 0.007575757575757576, 0.49074074074074076]\n",
      "[0.11315789473684211, 0.0025252525252525255, 0.159375]\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Unassign topic, reassign topic\n",
    "\n",
    "count_display = 0 # to control the displaying of the following probability\n",
    "for _ in range(NUM_ITER):\n",
    "    for doc_idx, doc in enumerate(processed_corpus):\n",
    "        # print(\"doc_idx: \", doc_idx)\n",
    "        for i in range(len(doc)):\n",
    "            word = doc[i]\n",
    "            # print(\"word: \", word)\n",
    "            # print(\"z[doc_idx][i][1]: \", z[doc_idx][i][1])\n",
    "            topic = z[doc_idx][i][1]\n",
    "            # print(\"topic: \", topic)\n",
    "\n",
    "            # In a document d, unassign a word from its topic\n",
    "            ndk[doc_idx][topic] -= 1\n",
    "            if ndk[doc_idx][topic] < 0:\n",
    "                del ndk[doc_idx][topic]\n",
    "            nwk[topic][word] -= 1\n",
    "            if nwk[topic][word] < 0:\n",
    "                del nwk[topic][word]\n",
    "            nk[topic] -= 1\n",
    "\n",
    "            # Assign w a new topic based on:\n",
    "            # a) The prevalence of each topic in the document:\n",
    "            #    Nb of times topic occurs in doc_idx + ALPHA / Total number of words in the doc_idx + K * ALPHA\n",
    "            # p_theta_d = (ndk[doc_idx][topic] + ALPHA) / (nd[doc_idx] + K * ALPHA)\n",
    "\n",
    "            # b) The prevalence of the word in each topic:\n",
    "            #    Nb of times word occurs in topic across corpus + BETA / Nb of words assigned to topic across corpus + n * BETA\n",
    "            # p_phi_k = (nwk[topic][word] + BETA) / (nk[topic] + n * BETA)\n",
    "\n",
    "            # Version 1:\n",
    "            p_z = [(ndk[doc_idx][k] + ALPHA) / (nd[doc_idx] + K * ALPHA) * \\\n",
    "                   (nwk[topic][word] + BETA) / (nk[k] + n * BETA) \\\n",
    "                   for k in range(K)]\n",
    "            \n",
    "            # Version 2:\n",
    "            p_z = [(ndk[doc_idx][k] + ALPHA) * (nwk[topic][word] + BETA) / (nk[k] + n * BETA) \\\n",
    "                   for k in range(K)]\n",
    "            \n",
    "            # Display the set of probabilities\n",
    "            if count_display < 5:\n",
    "                print(p_z)\n",
    "                count_display+=1\n",
    "\n",
    "            topic = random.choices(topic_list, weights=p_z, k=1)[0]\n",
    "            # topic = p_z.index(max(p_z)) # with the max\n",
    "            \n",
    "            # update n parameters\n",
    "            z[doc_idx][i] = word, topic\n",
    "            ndk[doc_idx][topic] += 1\n",
    "            nwk[topic][word] += 1\n",
    "            nk[topic] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL IN ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "np.random.seed(42)\n",
    "\n",
    "def lda_collapsed_gibs(corpus, num_iter=20, K=3, ALPHA=0.1, BETA=0.1):\n",
    "\n",
    "    # STEP 1: Randomly assign a topic number to every word in every document.\n",
    "\n",
    "    z = []\n",
    "    for _, doc in enumerate(corpus):\n",
    "        doc_with_topics = [(word, np.random.randint(low=0, high=K)) for word in doc]\n",
    "        z.append(doc_with_topics)\n",
    "\n",
    "\n",
    "    # STEP 2: Counting\n",
    "\n",
    "    # Number of words per topic per document\n",
    "    ndk = []\n",
    "    for _, doc_with_topic in enumerate(z):\n",
    "        topic_counts = defaultdict(int)\n",
    "        for word, topic in doc_with_topic:\n",
    "            topic_counts[topic] += 1\n",
    "        ndk.append(topic_counts)\n",
    "\n",
    "    # Number of times a word is assigned to a topic k across the corpus.\n",
    "    nwk = defaultdict(lambda: defaultdict(int))\n",
    "    for _, doc_with_topic in enumerate(z):\n",
    "        for word, topic in doc_with_topic:\n",
    "            nwk[topic][word] += 1\n",
    "    nwk = dict(nwk)\n",
    "\n",
    "    # Number of words per topics across the corpus\n",
    "    nk = defaultdict(int)\n",
    "    for dico in ndk:\n",
    "        for key, value in dico.items():\n",
    "            nk[key] += value\n",
    "\n",
    "    # Number of words per document\n",
    "    nd = defaultdict(int)\n",
    "    for i, dico in enumerate(ndk):\n",
    "        nd[i] = sum(dico.values())\n",
    "\n",
    "    # Number of words of the corpus:\n",
    "    n=0\n",
    "    for doc in corpus:\n",
    "        n+=len(doc)\n",
    "\n",
    "\n",
    "    # STEP 3. Unassign, reassign topics\n",
    "\n",
    "    for _ in range(NUM_ITER):\n",
    "        for doc_idx, doc in enumerate(corpus):\n",
    "            for i in range(len(doc)):\n",
    "                word = doc[i]\n",
    "                topic = z[doc_idx][i][1]\n",
    "\n",
    "                # In a document d, unassign a word from its topic\n",
    "                ndk[doc_idx][topic] -= 1\n",
    "                if ndk[doc_idx][topic] < 0:\n",
    "                    del ndk[doc_idx][topic]\n",
    "                nwk[topic][word] -= 1\n",
    "                if nwk[topic][word] < 0:\n",
    "                    del nwk[topic][word]\n",
    "                nk[topic] -= 1\n",
    "\n",
    "                # Assign w a new topic\n",
    "\n",
    "                p_z = [(ndk[doc_idx][k] + ALPHA) * (nwk[topic][word] + BETA) / (nk[k] + n * BETA) \\\n",
    "                    for k in range(K)]\n",
    "                \n",
    "                topic = random.choices(topic_list, weights=p_z, k=1)[0]\n",
    "                # topic = p_z.index(max(p_z)) # with the max\n",
    "                \n",
    "                # update n parameters\n",
    "                z[doc_idx][i] = word, topic\n",
    "                ndk[doc_idx][topic] += 1\n",
    "                nwk[topic][word] += 1\n",
    "                nk[topic] += 1\n",
    "\n",
    "    # STEP 4: Count the most frequent words per document\n",
    "    word_counts_per_doc = []\n",
    "    for doc_with_topics in z:\n",
    "        word_counts = Counter(word for word, _ in doc_with_topics)\n",
    "        word_counts_per_doc.append(word_counts)\n",
    "\n",
    "    return z, ndk, nwk, nk, word_counts_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0: [('orbit', 3), ('Moon', 3), ('lunar', 2), ('fuel', 2), ('gravitational', 2)]\n",
      "Document 1: [('Glad', 1), ('Griffin', 1), ('spend', 1), ('time', 1), ('engineer', 1)]\n",
      "Document 2: [('think', 3), ('cost', 3), ('SSTO', 3), ('large', 2), ('airliner', 2)]\n"
     ]
    }
   ],
   "source": [
    "z, ndk, nwk, nk, word_counts_per_doc = lda_collapsed_gibs(processed_corpus,ALPHA=20)\n",
    "\n",
    "# Print the most frequent words per document\n",
    "for doc_idx, word_counts in enumerate(word_counts_per_doc):\n",
    "    print(f\"Document {doc_idx}: {word_counts.most_common(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDICES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube - Alladin Persson](https://www.youtube.com/watch?v=aPRjj8i_6yE&t=161s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
