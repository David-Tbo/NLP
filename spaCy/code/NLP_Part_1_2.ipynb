{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing  \n",
    "author: D.Thébault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on NLP Demystified (YouTube) https://nlpdemystified.org\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Fundamentals of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Basic vectorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. **Reminders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He didn't want to pay $20 for the book.\n",
      "['He', 'did', \"n't\", 'want', 'to', 'pay', '$', '20', 'for', 'the', 'book', '.']\n",
      "He didn't\n",
      "[('He', 0), ('did', 1), (\"n't\", 2), ('want', 3), ('to', 4), ('pay', 5), ('$', 6), ('20', 7), ('for', 8), ('the', 9), ('book', 10), ('.', 11)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence = \"He didn't want to pay $20 for the book.\"\n",
    "doc = nlp(sentence)\n",
    "print(doc)\n",
    "print([t.text for t in doc]) # iterate over the Doc object\n",
    "print(doc[0:3]) # Slicing\n",
    "print([(t.text, t.i) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'did', \"n't\", 'want', 'to', 'pay', '$', '20', 'for', 'the', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "# Case Folding\n",
    "\n",
    "print([t.lower_ for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saw movie night . amused .\n"
     ]
    }
   ],
   "source": [
    "# Stop Words\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Print tokens and indicate if they are stop words\n",
    "\n",
    "sentence = \"I saw the movie last night. I was not amused.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "filtered_sentence = ' '.join([token.text for token in doc if not token.is_stop])\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "# Not possible with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Did', 'do'), (',', ','), ('Done', 'do'), (',', ','), ('Doing', 'do')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(\"Did, Done, Doing\")\n",
    "[(t.text, t.lemma_) for t in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'PROPN'), ('is', 'AUX'), ('a', 'DET'), ('watching', 'VERB'), ('an', 'DET'), ('old', 'ADJ'), ('movie', 'NOUN'), ('at', 'ADP'), ('a', 'DET'), ('cinema', 'NOUN'), ('.', 'PUNCT')]\n",
      "proper noun\n",
      "[('John', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('watching', 'VBG'), ('an', 'DT'), ('old', 'JJ'), ('movie', 'NN'), ('at', 'IN'), ('a', 'DT'), ('cinema', 'NN'), ('.', '.')]\n",
      "verb, past tense\n",
      "noun, proper singular\n"
     ]
    }
   ],
   "source": [
    "# Part-of-Speech tagging\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Loads the small English language model provided by SpaCy for NLP tasks.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Exemple of sentence\n",
    "sentence = \"John is a watching an old movie at a cinema.\"\n",
    "\n",
    "# Creating a Doc object that contains linguistic annotations for the text.\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# POS (course-grained) tags thanks to pos_ attribute\n",
    "print([(t.text, t.pos_) for t in doc])\n",
    "\n",
    "# to get a description for a POS tag, use spacy explain() method\n",
    "print(spacy.explain('PROPN'))\n",
    "\n",
    "# You can also have fine-grained tags with the attribute tag_\n",
    "# more details than with pos_ attribute (tense, type of pronoun...)\n",
    "print([(t.text, t.tag_) for t in doc])\n",
    "\n",
    "print(spacy.explain(\"VBD\"))\n",
    "print(spacy.explain(\"NNP\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Volkswagen', 'ORG'), ('is', ''), ('developping', ''), ('an', ''), ('electric', ''), ('sedan', ''), ('which', ''), ('could', ''), ('potentially', ''), ('come', ''), ('to', ''), ('America', 'GPE'), ('next', 'DATE'), ('fall', 'DATE'), ('.', '')]\n",
      "Countries, cities, states\n",
      "Companies, agencies, institutions, etc.\n",
      "Absolute or relative dates or periods\n",
      "[('Volkswagen', 'ORG'), ('America', 'GPE'), ('next', 'DATE'), ('fall', 'DATE')]\n",
      "[('Volkswagen', 'ORG'), ('America', 'GPE'), ('next fall', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognation (NER)\n",
    "\n",
    "s = \"Volkswagen is developping an electric sedan which could potentially come to America next fall.\"\n",
    "\n",
    "doc = nlp(s)\n",
    "\n",
    "# To access named entities we use here the spacy attribute ent_type_\n",
    "# others ways to make it are possible with Spacy.\n",
    "\n",
    "print([(t.text, t.ent_type_) for t in doc])\n",
    "\n",
    "print(spacy.explain('GPE'))\n",
    "print(spacy.explain('ORG'))\n",
    "print(spacy.explain('DATE'))\n",
    "\n",
    "# You can also check if a token is an entity before printing it by ckecking the attribute ent_type without underscore\n",
    "print([(t.text, t.ent_type_) for t in doc if t.ent_type != 0])\n",
    "\n",
    "# Another way is through the ents property of the Doc object itself (Note: \"next fall as a single entity this time\").\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[spaCy visualizers](https://spacy.io/usage/visualizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Volkswagen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is developping an electric sedan which could potentially come to \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    next fall\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# We need to set the 'jupyter' variable to True in order to ouput\n",
    "# the visualization directly. Otherwise, you'll get row HTML.\n",
    "# style = 'ent' for entity recognation.\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "\n",
    "print(spacy.explain('ORG'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She (nsubj) <-- enrolled\n",
      "enrolled (ROOT) <-- enrolled\n",
      "in (prep) <-- enrolled\n",
      "the (det) <-- course\n",
      "course (pobj) <-- in\n",
      "at (prep) <-- course\n",
      "the (det) <-- university\n",
      "university (pobj) <-- at\n",
      ". (punct) <-- enrolled\n",
      "nominal subject\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"c4655a28064a4903b0b1814bffe036e5-0\" class=\"displacy\" width=\"1450\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">She</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">enrolled</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">course</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">university.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4655a28064a4903b0b1814bffe036e5-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4655a28064a4903b0b1814bffe036e5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4655a28064a4903b0b1814bffe036e5-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4655a28064a4903b0b1814bffe036e5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4655a28064a4903b0b1814bffe036e5-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4655a28064a4903b0b1814bffe036e5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4655a28064a4903b0b1814bffe036e5-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4655a28064a4903b0b1814bffe036e5-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4655a28064a4903b0b1814bffe036e5-0-4\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4655a28064a4903b0b1814bffe036e5-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4655a28064a4903b0b1814bffe036e5-0-5\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,89.5 1270.0,89.5 1270.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4655a28064a4903b0b1814bffe036e5-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,179.0 L1112,167.0 1128,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4655a28064a4903b0b1814bffe036e5-0-6\" stroke-width=\"2px\" d=\"M945,177.0 C945,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4655a28064a4903b0b1814bffe036e5-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dependency Parsing\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the english language model of SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sentence to analyse\n",
    "sentence = \"She enrolled in the course at the university.\"\n",
    "\n",
    "# Analyze the sentence\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Display the dependency relations\n",
    "for token in doc:\n",
    "    print(f\"{token.text} ({token.dep_}) <-- {token.head.text}\")\n",
    "\n",
    "print(spacy.explain(\"nsubj\"))\n",
    "\n",
    "# Let's visualize a dependency parse\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches:  ['book a hotel', 'book a hotel room']\n",
      "phrase: I, root head: want\n",
      "phrase: a flight and hotel room, root head: book\n",
      "phrase: Berlin, root head: in\n"
     ]
    }
   ],
   "source": [
    "# The general Matcher is one of multiple matcher objects\n",
    "# included with spaCy.\n",
    "from spacy.matcher import Matcher\n",
    "# We initialize the Matcher with the spaCy vocab object, which contains\n",
    "# words along with their labels and tags.\n",
    "matcher = Matcher(nlp.vocab)\n",
    "s = \"I want to book a hotel room.\"\n",
    "doc = nlp(s)\n",
    "# Patterns are expressed as an ordered sequence. \n",
    "# Here, we're looking to match occurences starting with a 'book' string followed by \n",
    "# a determiner (DET) POS tag such as \"the\",\"and\" , then a noun POS tag.\n",
    "# The OP key marks the match as optional in some way.\n",
    "\n",
    "# Here, the DET POS (marked with '?') will match 0 or 1 times (i.e. the determiner is optional), and \n",
    "# the NOUN POS (marked with '+') will match 1 or more times (i.e., at least one noun is required).\n",
    "# See this link for more information.\n",
    "# https://spacy.io/usage/rule-based-matching#quantifiers\n",
    "\n",
    "pattern = [\n",
    "    {'TEXT': 'book'},\n",
    "    {'POS': 'DET', 'OP': '?'},\n",
    "    {'POS': 'NOUN', 'OP': '+'}\n",
    "]\n",
    "\n",
    "# So, the pattern will match sequences that start with the word \"book\", \n",
    "# optionally followed by a determiner, and then followed by one or more nouns.\n",
    "# We give our pattern a label and pass it to the matcher.\n",
    "matcher.add('USER_INTENT', [pattern])\n",
    "\n",
    "# Run the matcher over the doc.\n",
    "matches = matcher(doc)\n",
    "\n",
    "# For each match, the matcher returns a tuple specifying a match id, start,\n",
    "# and end of the match.\n",
    "print(\"Matches: \", [doc[start:end].text for match_id, start, end in matches])\n",
    "\n",
    "doc = nlp(\"I want to book a flight and hotel room in Berlin.\")\n",
    "for noun_phrase in doc.noun_chunks:\n",
    "  print(\"phrase: {}, root head: {}\".format(noun_phrase, noun_phrase.root.head))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the ML algorith we need to translate our pre-processed text into vectors.**  \n",
    "\n",
    "**This is called <u>Vectorization</u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector is simply an array or a list of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work our text with ML algorithm we need  \n",
    "to turn text into numbers and measuring similarity between documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature:**  \n",
    "\n",
    "Any property in your data you think is useful for making predictions or explaining some relationship.\n",
    "\n",
    "Exemples in NLP:\n",
    "- word count\n",
    "- document age\n",
    "- author id\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, we want a **Matrix** where:  \n",
    "\n",
    "- Each row represents a document called an instance or a feature vector,\n",
    "\n",
    "- Each column is a feature.  \n",
    "\n",
    "A dcoument can be a sentence, a tweet, an entire book..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Bag-of-Words** (BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describing documents by word occurences**  \n",
    "\n",
    "Basic idea: if two documents share the same vocabulary, the more likely they belong to the same class.  \n",
    "\n",
    "This is called bag-of-words because of the lack of order of the words, no grammar taking into account, no syntaxe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A corpus of sentences.\n",
    "corpus = [\n",
    "  \"Red Bull drops hint on F1 engine.\",\n",
    "  \"Honda exits F1, leaving F1 partner Red Bull.\",\n",
    "  \"Hamilton eyes record eighth F1 title.\",\n",
    "  \"Aston Martin announces sponsor.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Frequency BOW</u>  \n",
    "\n",
    "- Each column of the matrix represents a word in the vocabulary with its frequence in the document.\n",
    "\n",
    "- Each row is a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer takes a collection of text documents and creates \n",
    "# a matrix of token counts\n",
    "\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 24 stored elements and shape (4, 20)>\n",
      "  Coords\tValues\n",
      "  (0, 17)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 8)\t2\n",
      "  (1, 11)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 15)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 19)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 13)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 18)\t1\n"
     ]
    }
   ],
   "source": [
    "# The fit_transform method does two things:\n",
    "# 1. It learns a vocabulary dictionary from the corpus.\n",
    "# 2. It returns a matrix where each row represents a document and \n",
    "# each column represents a token\n",
    "\n",
    "bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['announces' 'aston' 'bull' 'drops' 'eighth' 'engine' 'exits' 'eyes' 'f1'\n",
      " 'hamilton' 'hint' 'honda' 'leaving' 'martin' 'on' 'partner' 'record'\n",
      " 'red' 'sponsor' 'title']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'red': 17,\n",
       " 'bull': 2,\n",
       " 'drops': 3,\n",
       " 'hint': 10,\n",
       " 'on': 14,\n",
       " 'f1': 8,\n",
       " 'engine': 5,\n",
       " 'honda': 11,\n",
       " 'exits': 6,\n",
       " 'leaving': 12,\n",
       " 'partner': 15,\n",
       " 'hamilton': 9,\n",
       " 'eyes': 7,\n",
       " 'record': 16,\n",
       " 'eighth': 4,\n",
       " 'title': 19,\n",
       " 'aston': 1,\n",
       " 'martin': 13,\n",
       " 'announces': 0,\n",
       " 'sponsor': 18}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View features (tokens).\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# View vocabulary dictionary.\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Binary BOW</u>  \n",
    "\n",
    "- Each column of the matrix represents a word in the vocabulary (0 absent, 1 present)\n",
    "\n",
    "- Each row is a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer supports using a custom tokenizer.  \n",
    "For every document, it will call your tokenizer and  \n",
    "expect a list of tokens returned.  \n",
    "We'll create a simple callback below which has spaCy  \n",
    "tokenize and filter tokens, and then return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, we start by importing spaCy and loading a statistical model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a tokenizer callback using spaCy under the hood. Here, we tokenize\n",
    "# the passed-in text and return the tokens, filtering out punctuation.\n",
    "def spacy_tokenizer(doc):\n",
    "  return [t.text for t in nlp(doc) if not t.is_punct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we instantiate CountVectorizer with our custom tokenizer \n",
    "(spacy_tokenizer), turn off case-folding,  \n",
    "and also set the binary parameter to True  \n",
    "so we simply get 1s and 0s marking token presence rather than token frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True)\n",
    "bow = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aston' 'Bull' 'F1' 'Hamilton' 'Honda' 'Martin' 'Red' 'announces' 'drops'\n",
      " 'eighth' 'engine' 'exits' 'eyes' 'hint' 'leaving' 'on' 'partner' 'record'\n",
      " 'sponsor' 'title']\n"
     ]
    }
   ],
   "source": [
    "# View features (tokens).\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Red Bull drops hint on F1 engine.',\n",
       " 'Honda exits F1, leaving F1 partner Red Bull.',\n",
       " 'Hamilton eyes record eighth F1 title.',\n",
       " 'Aston Martin announces sponsor.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 24 stored elements and shape (4, 20)>\n",
      "  Coords\tValues\n",
      "  (0, 6)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 10)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 16)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 19)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 18)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a dense array representation of our sparse matrix, use toarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dense representation like we saw in the slides.\n",
      "[[0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0]\n",
      " [0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0]\n",
      " [0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1]\n",
      " [1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0]]\n",
      "\n",
      "Indexing and slicing.\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 7 stored elements and shape (1, 20)>\n",
      "  Coords\tValues\n",
      "  (0, 6)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 10)\t1\n",
      "\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 14 stored elements and shape (2, 20)>\n",
      "  Coords\tValues\n",
      "  (0, 6)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 10)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 16)\t1\n"
     ]
    }
   ],
   "source": [
    "print('A dense representation like we saw in the slides.')\n",
    "print(bow.toarray())\n",
    "print()\n",
    "print('Indexing and slicing.')\n",
    "print(bow[0])\n",
    "print()\n",
    "print(bow[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With BOW we have shifted from a sequence of symbols to points in a multidimensional space that encodes some meaning of the text.  \n",
    "\n",
    "Each feature vector in our BOW is now a point in this multidimensional space.  \n",
    "This is called a **Vector Space Model** (VSM).  \n",
    "\n",
    "Two documents which have similar vocabulary should be closer together in this space.  \n",
    "\n",
    "This allows to measure similarity which is useful for:\n",
    "\n",
    "- Relevance ranking,\n",
    "- Plagiarism detection,\n",
    "- Document classification\n",
    "- ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cosine similarity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we measure similarity ?  \n",
    "\n",
    "The more two vectors share the same direction and the same magnitude, the more similar they are.\n",
    "\n",
    "$a . b = \\sum_{i=1}^{n}a_1b_1 + a_2b_2 + \\ldots + a_nb_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "a.dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But high frequency words will lead to larger dot products...  \n",
    "To avoid this we normalize by vector length: $||v|| = \\sqrt{\\sum_{i=1}^nv_i^2}$  \n",
    "Knows also as the $L^2$ norm or Euclidean norm.  \n",
    "\n",
    "$ \\frac{a . b}{||a|| ||b||} = cos(\\theta)$  \n",
    "\n",
    "With $\\theta$ the angle between the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our Cosine similarity function:\n",
    "def cos_similar(a,b) -> float:\n",
    "    \"\"\"Compute Cosine similarity between two vectors.\n",
    "    Inputs:\n",
    "        a, b: vectors\n",
    "    Returns,\n",
    "        a float\n",
    "    \"\"\"\n",
    "    a_norm = (a.transpose().dot(a))**0.5\n",
    "    b_norm = (b.transpose().dot(b))**0.5\n",
    "    return a.dot(b) / (a_norm * b_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746318461970762\n"
     ]
    }
   ],
   "source": [
    "print(cos_similar(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9746318461970762"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or use scipy spatial\n",
    "from scipy import spatial\n",
    "1 - spatial.distance.cosine(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of $cos(\\theta)$ ranges from 0 to 1.\n",
    "- 1 when the vectors point in the same direction,\n",
    "- 0 when the vectors are orhogonal (dissimilar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(bow[0].toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Red Bull drops hint on F1 engine.', 'Honda exits F1, leaving F1 partner Red Bull.', 'Hamilton eyes record eighth F1 title.', 'Aston Martin announces sponsor.']\n",
      "Doc 1 vs Doc 2: 0.4285714285714286\n",
      "Doc 1 vs Doc 3: 0.15430334996209194\n",
      "Doc 1 vs Doc 4: 0.0\n"
     ]
    }
   ],
   "source": [
    "# The cosine method expects array_like inputs, so we need to generate\n",
    "# arrays from our sparse matrix.\n",
    "doc1_vs_doc2 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[1].toarray()[0])\n",
    "doc1_vs_doc3 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[2].toarray()[0])\n",
    "doc1_vs_doc4 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[3].toarray()[0])\n",
    "\n",
    "print(corpus)\n",
    "\n",
    "print(f\"Doc 1 vs Doc 2: {doc1_vs_doc2}\")\n",
    "print(f\"Doc 1 vs Doc 3: {doc1_vs_doc3}\")\n",
    "print(f\"Doc 1 vs Doc 4: {doc1_vs_doc4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is using scikit-learn's ```cosine_similarity``` which  \n",
    "computes the metric between multiple vectors.  \n",
    "Here, we pass it our BOW and get a matrix of cosine similarities between each document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.42857143 0.15430335 0.        ]\n",
      " [0.42857143 1.         0.15430335 0.        ]\n",
      " [0.15430335 0.15430335 1.         0.        ]\n",
      " [0.         0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# cosine_similarity can take either array-likes or sparse matrices.\n",
    "print(cosine_similarity(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawbacks of BOW:\n",
    "\n",
    "- Does not capture similarity between synonyms.\n",
    "- No way to handle Out-of-Vocabulary (OOV) words.\n",
    "- Creates sparse vectors which can be inefficient\n",
    "- Word order information is lots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. **N-grams**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chuncks of continuous tokens.  \n",
    "\n",
    "2-grams or bigram has two tokens per chunck.  \n",
    "3-grams or trigram has three tokens per chunck.  \n",
    "...\n",
    "\n",
    "Exemple of tokenization into bigram:  \n",
    "\n",
    "\"Barcelona beats Chelsea\" => [\"Barcelona beats\", \"beats Chelsea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```CountVectorizer()``` includes an ```ngram_range``` parameter to generate different n-grams.  \n",
    "n_gram range is specified using a minimum and maximum range.  \n",
    "By default, n_gram range is set to (1, 1) which generates unigrams.  \n",
    "Setting it to (1, 2) generates both unigrams and bigrams.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aston' 'Aston Martin' 'Bull' 'Bull drops' 'F1' 'F1 engine' 'F1 leaving'\n",
      " 'F1 partner' 'F1 title' 'Hamilton' 'Hamilton eyes' 'Honda' 'Honda exits'\n",
      " 'Martin' 'Martin announces' 'Red' 'Red Bull' 'announces'\n",
      " 'announces sponsor' 'drops' 'drops hint' 'eighth' 'eighth F1' 'engine'\n",
      " 'exits' 'exits F1' 'eyes' 'eyes record' 'hint' 'hint on' 'leaving'\n",
      " 'leaving F1' 'on' 'on F1' 'partner' 'partner Red' 'record'\n",
      " 'record eighth' 'sponsor' 'title']\n",
      "Number of features: 40\n",
      "{'Red': 15, 'Bull': 2, 'drops': 19, 'hint': 28, 'on': 32, 'F1': 4, 'engine': 23, 'Red Bull': 16, 'Bull drops': 3, 'drops hint': 20, 'hint on': 29, 'on F1': 33, 'F1 engine': 5, 'Honda': 11, 'exits': 24, 'leaving': 30, 'partner': 34, 'Honda exits': 12, 'exits F1': 25, 'F1 leaving': 6, 'leaving F1': 31, 'F1 partner': 7, 'partner Red': 35, 'Hamilton': 9, 'eyes': 26, 'record': 36, 'eighth': 21, 'title': 39, 'Hamilton eyes': 10, 'eyes record': 27, 'record eighth': 37, 'eighth F1': 22, 'F1 title': 8, 'Aston': 0, 'Martin': 13, 'announces': 17, 'sponsor': 38, 'Aston Martin': 1, 'Martin announces': 14, 'announces sponsor': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, \n",
    "                             lowercase=False, \n",
    "                             binary=True, \n",
    "                             ngram_range=(1,2))\n",
    "\n",
    "bigrams = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('Number of features: {}'.format(len(vectorizer.get_feature_names_out())))\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aston Martin' 'Bull drops' 'F1 engine' 'F1 leaving' 'F1 partner'\n",
      " 'F1 title' 'Hamilton eyes' 'Honda exits' 'Martin announces' 'Red Bull'\n",
      " 'announces sponsor' 'drops hint' 'eighth F1' 'exits F1' 'eyes record'\n",
      " 'hint on' 'leaving F1' 'on F1' 'partner Red' 'record eighth']\n",
      "{'Red Bull': 9, 'Bull drops': 1, 'drops hint': 11, 'hint on': 15, 'on F1': 17, 'F1 engine': 2, 'Honda exits': 7, 'exits F1': 13, 'F1 leaving': 3, 'leaving F1': 16, 'F1 partner': 4, 'partner Red': 18, 'Hamilton eyes': 6, 'eyes record': 14, 'record eighth': 19, 'eighth F1': 12, 'F1 title': 5, 'Aston Martin': 0, 'Martin announces': 8, 'announces sponsor': 10}\n"
     ]
    }
   ],
   "source": [
    "# Setting n_gram range to (2, 2) generates only bigrams.\n",
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True, ngram_range=(2,2))\n",
    "bigrams = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Create a spacy_tokenizer callback which takes a string and returns\n",
    "# a list of tokens (each token's text) with punctuation filtered out.\n",
    "#\n",
    "corpus = [\n",
    "  \"Students use their GPS-enabled cellphones to take birdview photographs of a land in order to find specific danger points such as rubbish heaps.\",\n",
    "  \"Teenagers are enthusiastic about taking aerial photograph in order to study their neighbourhood.\",\n",
    "  \"Aerial photography is a great way to identify terrestrial features that aren’t visible from the ground level, such as lake contours or river paths.\",\n",
    "  \"During the early days of digital SLRs, Canon was pretty much the undisputed leader in CMOS image sensor technology.\",\n",
    "  \"Syrian President Bashar al-Assad tells the US it will 'pay the price' if it strikes against Syria.\"\n",
    "]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_tokenizer(doc):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Initialize a CountVectorizer object and set it to use\n",
    "# your spacy_tokenizer with lower-casing off and to create a binary BOW.\n",
    "#\n",
    "\n",
    "# Instantiate a CountVectorizer object called 'vectorizer'.\n",
    "\n",
    "\n",
    "# Create a binary BOW from the corpus using your CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The string below is a whole paragraph. We want to create another\n",
    "# binary BOW but using the vocabulary of our *current* CountVectorizer. This means\n",
    "# that words in this paragraph which AREN'T already in the vocabulary won't be\n",
    "# represented. This is to illustrate how BOW can't handle out-of-vocabulary words\n",
    "# unless you rebuild your whole vocabulary. Still, we'll see that if there's\n",
    "# enough overlapping vocabulary, some similarity can still be picked up.\n",
    "#\n",
    "# Note that we call 'transform' only instead of 'fit_transform' because the\n",
    "# fit step (i.e. vocabulary build) is already done and we don't want to re-fit here.\n",
    "#\n",
    "s = [\"Teenagers take aerial shots of their neighbourhood using digital cameras sitting in old bottles which are launched via kites - a common toy for children living in the favelas. They then use GPS-enabled smartphones to take pictures of specific danger points - such as rubbish heaps, which can become a breeding ground for mosquitoes carrying dengue fever.\"]\n",
    "new_bow = vectorizer.transform(s)\n",
    "\n",
    "#\n",
    "# EXERCISE: using the pairwise cosine_similarity method from sklearn,\n",
    "# calculate the similarities between each document from the corpus against\n",
    "# this new document (new_bow). HINT: You can pass two parameters to\n",
    "# cosine_similarity in this case. See the docs:\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html#scipy.spatial.distance.cosine\n",
    "#\n",
    "# Which document is the most similar? Which is the least similar? Do the results make sense\n",
    "# based on what you see?\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: In spacy_tokenizer, instead of returning the plain text,\n",
    "# return the lemma_ attribute instead. How do the cosine similarity\n",
    "# results differ? What if you filter out stop words as well?\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Machine Learning algorithms vs models, evaluation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps into Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying text using Naive Bayes; evaluation with precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically finding topics in documents using Latent Dirichlet Allocation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
