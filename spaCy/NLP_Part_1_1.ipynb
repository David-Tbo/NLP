{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing  \n",
    "author: D.Th√©bault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on NLP Demystified (YouTube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Fundamentals of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0. **Installation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install pyenv:   \n",
    "\n",
    "\n",
    "```{python}\n",
    "brew update\n",
    "brew install pyenv\n",
    "```  \n",
    "\n",
    "Install Python 3.11 with 'pyenv':  \n",
    "\n",
    "```{python}\n",
    "pyenv install 3.11.0\n",
    "```\n",
    "\n",
    "Define Python 3.11 as local version:  \n",
    "\n",
    "```{python}\n",
    "pyenv local 3.11.0\n",
    "```  \n",
    "\n",
    "Create a virtual environment with Python 3.11:  \n",
    "```{python}\n",
    "virtualenv -p $(pyenv which python3.11) myenv\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env\n",
    "#!conda create --name env python=3.11\n",
    "#!conda activate env\n",
    "\n",
    "# Spacy\n",
    "#!conda uninstall -y numpy h5py spacy\n",
    "#!conda install numpy h5py spacy\n",
    "#!conda uninstall -y numpy h5py spacy\n",
    "#!conda install --upgrade numpy h5py\n",
    "#!conda install ipykernel\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'punkt' :  \n",
    "\n",
    "    Tokenizer which use a model (probability) to determine if for instance a . means the ending point or Dr.Smith\n",
    "\n",
    "- 'stopwords' :   \n",
    "\n",
    "- 'wordnet' :  \n",
    "- 'omw-1.4' :   \n",
    "- 'averaged_perceptron_tagger' :   \n",
    "- 'maxent_ne_chunker' : NER  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. **Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking our documents into tokens (words, punctuation, numbers).  \n",
    "\n",
    "It is the first step.  \n",
    "\n",
    "Tokenization = Documents $\\Rightarrow$ sentences $\\Rightarrow$ tokens (words, punctuation, numbers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', \"didn't\", 'want', 'to', 'pay', '$20', 'for', 'the', 'book.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"He didn't want to pay $20 for the book.\".split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Problems:</u> \"$20\" and \"book.\" are not separated. We could you regexp() but how do we manage \"N.Y.C.\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definitions:**  \n",
    "\n",
    "- **<u>word</u>**: the smallest unit of <u>speech</u> that carries some meaning <u>on its own</u>.\n",
    "\n",
    "    \"full moon\" one word or two words ? Should be map to one meaning or not ? In our definition it is two words.\n",
    "\n",
    "- **<u>Morpheme</u>**: the smallest unit of speech which has a meaning, but doesn't necessarily stand on its own.  \n",
    "\n",
    "    Examples suffixes or prefixes such as : -ing, re-, pre-, un-   \n",
    "\n",
    "- **<u>Grapheme</u>**: the smallest ufuncitonal unit of a writing system. In English that's letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "He didn't want to pay $20 for the book."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence = \"He didn't want to pay $20 for the book.\"\n",
    "doc = nlp(sentence)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'did', \"n't\", 'want', 'to', 'pay', '$', '20', 'for', 'the', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "# We can iterate over this Doc object and view the tokens.\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n"
     ]
    }
   ],
   "source": [
    "# We can view an individual token by indexing into the Doc object.\n",
    "print(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# A Doc object is a container of other objects, namely Token and Span objects.\n",
    "print(type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He didn't\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "# Slicing a Doc object returns a Span object.\n",
    "print(doc[0:3])\n",
    "print(type(doc[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 0), ('did', 1), (\"n't\", 2), ('want', 3), ('to', 4), ('pay', 5), ('$', 6), ('20', 7), ('for', 8), ('the', 9), ('book', 10), ('.', 11)]\n"
     ]
    }
   ],
   "source": [
    "# Access a token's index in a sentence.\n",
    "print([(t.text, t.i) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He didn't want to pay $20 for the book.\n"
     ]
    }
   ],
   "source": [
    "# You can view the original input like so:\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn more about the Token and Span objects here:  \n",
    "https://spacy.io/api/token  \n",
    "https://spacy.io/api/span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. **Case Folding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower - or upper casing all tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definitions:**  \n",
    "\n",
    "- **<u>Vocabulary</u>**: The set of all **unique** tokens in a corpus.\n",
    "\n",
    "In the following exemple, case folding can lead to an information loss (the noun Cook becomes cook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr.', 'cook', 'went', 'into', 'the', 'kitchen', 'to', 'cook', 'dinner', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence = \"Mr. cook went into the kitchen to cook dinner.\"\n",
    "doc = nlp(sentence)\n",
    "print([t.lower_ for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mr., 'cook', 'went', 'into', 'the', 'kitchen', 'to', 'cook', 'dinner', '.']\n"
     ]
    }
   ],
   "source": [
    "# We skip lowering if the word starts the sentence.doc\n",
    "print([t.lower_ if not t.is_sent_start else t for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. **Stop Word Removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing words which occur frequently but carry little information.  \n",
    "\n",
    "{the, a, of, an, this, that, ...}  \n",
    "\n",
    "Be carefull in the following exemple, this can lead to a counter-sense.  \n",
    "It is up to you to use stopwords - generally we use it.  \n",
    "stopwords could also be parametrized for your own purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: I, Is Stop Word: True\n",
      "Token: saw, Is Stop Word: False\n",
      "Token: the, Is Stop Word: True\n",
      "Token: movie, Is Stop Word: False\n",
      "Token: last, Is Stop Word: True\n",
      "Token: night, Is Stop Word: False\n",
      "Token: ., Is Stop Word: False\n",
      "Token: I, Is Stop Word: True\n",
      "Token: was, Is Stop Word: True\n",
      "Token: not, Is Stop Word: True\n",
      "Token: amused, Is Stop Word: False\n",
      "Token: ., Is Stop Word: False\n",
      "saw movie night . amused .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence = \"I saw the movie last night. I was not amused.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Print tokens and indicate if they are stop words\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text}, Is Stop Word: {token.is_stop}\")\n",
    "\n",
    "#¬†Remove stop words\n",
    "filtered_sentence = ' '.join([token.text for token in doc if not token.is_stop])\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. **Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing word suffixes (and sometimes prefixes)  \n",
    "In French, Stemming (racine orthographique)  \n",
    "\n",
    "{ing, s, y, ed, ...}  \n",
    "\n",
    "The goal is to reduce a word to some base form.  \n",
    "Typically done through an algorithm, the most famous of which is Porter's algorithm.  \n",
    "\n",
    "Banking => Bank  \n",
    "Banks => Bank  \n",
    "\n",
    "Note that a stemmed word may not be a valid word.  \n",
    "\n",
    "\"Analysis\" => \"Analysi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. **Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce a word down to its lemma, or dictionary form.\n",
    "(more sophisticated than stemming)  \n",
    "\n",
    "In French, Lemmatization (racine s√©mantique)  \n",
    "\n",
    "\"Did\", \"Done\", \"Doing\" => \"Do\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Did', 'do'), (',', ','), ('Done', 'do'), (',', ','), ('Doing', 'do')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "doc = nlp(\"Did, Done, Doing\")\n",
    "[(t.text, t.lemma_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. **POS Tagging**\n",
    "\n",
    "Part-of-Speech (PoS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying how a word is used in a sentence.  \n",
    "- NOUN  \n",
    "- VERB  \n",
    "- ADJECTIVE\n",
    "- ...\n",
    "\n",
    "_Example_  \n",
    "\"I want to book a hotel room.\"  $\\rightarrow$ book : POS Tagger = VERB  \n",
    "\n",
    "\"I left the book in the hotel room.\" $\\rightarrow$ book : POS Tagger = NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging assigns each word in a sentence its corresponding POS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'PROPN'),\n",
       " ('is', 'AUX'),\n",
       " ('a', 'DET'),\n",
       " ('watching', 'VERB'),\n",
       " ('an', 'DET'),\n",
       " ('old', 'ADJ'),\n",
       " ('movie', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('cinema', 'NOUN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Loads the small English language model provided by SpaCy for NLP tasks.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Exemple of sentence\n",
    "sentence = \"John is a watching an old movie at a cinema.\"\n",
    "\n",
    "# Creating a Doc object that contains linguistic annotations for the text.\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# POS (course-grained) tags thanks to pos_ attribute\n",
    "[(t.text, t.pos_) for t in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get a description for a POS tag, use spacy explain() method\n",
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('watching', 'VBG'),\n",
       " ('an', 'DT'),\n",
       " ('old', 'JJ'),\n",
       " ('movie', 'NN'),\n",
       " ('at', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('cinema', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also have fine-grained tags with the attribute tag_\n",
    "# more details than with pos_ attribute (tense, type of pronoun...)\n",
    "[(t.text, t.tag_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NNP** refers specifically to a singular pronoun, and **VDB** is a part tense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verb, past tense\n",
      "noun, proper singular\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"VBD\"))\n",
    "print(spacy.explain(\"NNP\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging can use:  \n",
    "\n",
    "- <u>Linguistic Rules</u>:  \n",
    "Predefined linguistic rules such as if the word ending by \"ing\" it is a VERB or and ADJ.  \n",
    "\n",
    "- <u>Dictionaries</u>:  \n",
    "This method is limited because cannot manage unknown words or contextual ambiguities.  \n",
    "\n",
    "- <u>Hidden Markov Model (HMM)</u>: \n",
    "HMM uses the sequence of word as a Markov Chain where each state represents a grammatical category.  \n",
    "The model learns the probabilities of transitions between states (gramatical category) and the emission probabilities   \n",
    "(probabilities that a words is emited by a grammatical category).  \n",
    "HMM is a statistical model.  \n",
    "\n",
    "- <u>Conditional Random Field (CRF)</u>:  \n",
    "CRF is a statistical model.  \n",
    "\n",
    "- <u>Maximum Entropy model (MaxEnt)</u>:  \n",
    "MaxEnt models use contextual features to predict the grammatical category of a word.  \n",
    "These features can include the preceding and following words, suffixes, prefixes, etc.  \n",
    "The model learns the weights of these features to maximize entropy.  \n",
    "MaxEnt is a statistical model.  \n",
    "\n",
    "- <u>Recurrent Neural Networks (RNN) or Long Short-Term Memory networks (LSTMs)</u>:  \n",
    "These models can capture long-term dependencies in the text and are capable of handling contextual ambiguities.\n",
    "\n",
    "- <u>Transformers</u>:  \n",
    "Such as BERT. These models use self-supervised attention to capture complex relationships between words in the text.\n",
    "\n",
    "- <u>Hybrid Algorithms</u>:  \n",
    "Some POS tagging systems combine linguistic rules and statistical models to improve accuracy.  \n",
    "For example, a system may use rules to handle specific cases and statistical models to handle general cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. **Named Entity Recognition (NER)**\n",
    "\n",
    "Tagging named (\"real-world\") entities.  \n",
    "\n",
    "{a person, a locationn, an organiation, ...}\n",
    "\n",
    "Named Entity: roughly anything that can be referred by a proper name.  \n",
    "They often have a Proper Noun (PROPN) POS tag.  \n",
    "\n",
    "It will help to identify in a corpus an organization, an entity or a person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER can be seen as a sequence labelling tasks such as BIO (**B**eginning of entity, **I**nside of entity, **O**utstide of entity).  \n",
    "\n",
    "\"Alexander Hamilton was born in Charleston, Nevis\"  \n",
    "\n",
    "Alexander $\\Rightarrow$ B-PER (Beginning of person entity)  \n",
    "Hamliton $\\Rightarrow$ I-PER (Inside (continuation) of person entity)  \n",
    "was $\\Rightarrow$ O (Outside of entity.)  \n",
    "born $\\Rightarrow$ O  \n",
    "in $\\Rightarrow$ O  \n",
    "Charleston $\\Rightarrow$ B-GPE  \n",
    ", $\\Rightarrow$ O  \n",
    "Nevis $\\Rightarrow$ B-GPE  \n",
    ". $\\Rightarrow$ O  \n",
    "\n",
    "<u>NB</u>: BIO is not a linguistic rule but a convention of annotation used to structure training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Volkswagen', 'ORG'),\n",
       " ('is', ''),\n",
       " ('developping', ''),\n",
       " ('an', ''),\n",
       " ('electric', ''),\n",
       " ('sedan', ''),\n",
       " ('which', ''),\n",
       " ('could', ''),\n",
       " ('potentially', ''),\n",
       " ('come', ''),\n",
       " ('to', ''),\n",
       " ('America', 'GPE'),\n",
       " ('next', 'DATE'),\n",
       " ('fall', 'DATE'),\n",
       " ('.', '')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NER With spacy\n",
    "\n",
    "s = \"Volkswagen is developping an electric sedan which could potentially come to America next fall.\"\n",
    "\n",
    "doc = nlp(s)\n",
    "\n",
    "# To access named entities we use here the spacy attribute ent_type_\n",
    "# others ways to make it are possible with Spacy.\n",
    "\n",
    "[(t.text, t.ent_type_) for t in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n",
      "Companies, agencies, institutions, etc.\n",
      "Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('GPE'))\n",
    "print(spacy.explain('ORG'))\n",
    "print(spacy.explain('DATE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Volkswagen', 'ORG'), ('America', 'GPE'), ('next', 'DATE'), ('fall', 'DATE')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also check if a token is an entity before printing it by ckecking the attribute ent_type without underscore\n",
    "[(t.text, t.ent_type_) for t in doc if t.ent_type != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Volkswagen', 'ORG'), ('America', 'GPE'), ('next fall', 'DATE')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way is through the ents property of the Doc object itself (Note: \"next fall as a single entity this time\").\n",
    "[(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[spaCy visualizers](https://spacy.io/usage/visualizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Volkswagen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is developping an electric sedan which could potentially come to \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    next fall\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# We need to set the 'jupyter' variable to True in order to ouput\n",
    "# the visualization directly. Otherwise, you'll get row HTML.\n",
    "# style = 'ent' for entity recognation.\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ridley Scott\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " directed the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Martian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = \"Ridley Scott directed the Martian.\"\n",
    "doc = nlp(s)\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nationalities or religious or political groups'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NORP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8. **Parsing**\n",
    "\n",
    "Determining the syntactic structure of a sentence. \n",
    "\n",
    "{_sujet_, _verbe_, _COD_, _COI_, _compl√©ment circonstanciel_, _adjectifs_, _adverbes_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Consistency Parsing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creates a parse tree.**\n",
    "\n",
    "For that Consistency Parsing uses a Context-Free Grammar (CFG):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules for the sentences (S),  \n",
    "the groupes nominaux (NP),  \n",
    "the groupes verbaux (VP),  \n",
    "the groupes pr√©positionnels (PP),  \n",
    "the verbes (V),  \n",
    "the d√©terminants (Det),  \n",
    "the noms (N), and  \n",
    "the pr√©positions (P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CFG encompasses \"Production Rules\" (line 1 to 3) and Lexicon (line 4 to 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree shows the syntactic structure of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Dependency Parsing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She (nsubj) <-- enrolled\n",
      "enrolled (ROOT) <-- enrolled\n",
      "in (prep) <-- enrolled\n",
      "the (det) <-- course\n",
      "course (pobj) <-- in\n",
      "at (prep) <-- course\n",
      "the (det) <-- university\n",
      "university (pobj) <-- at\n",
      ". (punct) <-- enrolled\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the english language model of SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sentence to analyse\n",
    "sentence = \"She enrolled in the course at the university.\"\n",
    "\n",
    "# Analyze the sentence\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Display the dependency relations\n",
    "for token in doc:\n",
    "    print(f\"{token.text} ({token.dep_}) <-- {token.head.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"20f1d509aaa443239caf2617f7d66f9e-0\" class=\"displacy\" width=\"1450\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">She</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">enrolled</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">course</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">university.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-20f1d509aaa443239caf2617f7d66f9e-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-20f1d509aaa443239caf2617f7d66f9e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-20f1d509aaa443239caf2617f7d66f9e-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-20f1d509aaa443239caf2617f7d66f9e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-20f1d509aaa443239caf2617f7d66f9e-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-20f1d509aaa443239caf2617f7d66f9e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-20f1d509aaa443239caf2617f7d66f9e-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-20f1d509aaa443239caf2617f7d66f9e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-20f1d509aaa443239caf2617f7d66f9e-0-4\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-20f1d509aaa443239caf2617f7d66f9e-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-20f1d509aaa443239caf2617f7d66f9e-0-5\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,89.5 1270.0,89.5 1270.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-20f1d509aaa443239caf2617f7d66f9e-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,179.0 L1112,167.0 1128,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-20f1d509aaa443239caf2617f7d66f9e-0-6\" stroke-width=\"2px\" d=\"M945,177.0 C945,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-20f1d509aaa443239caf2617f7d66f9e-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#¬†With spaCy\n",
    "import spacy\n",
    "\n",
    "# Let's visualize a dependency parse\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take the above relation \"She\" <- \"enrolled\". It is a Nominal subject relationship where:  \n",
    "- The child or dependent is \"She\". \n",
    "- The head or governor is \"enrolled\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/api/annotation#dependency-parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nominal subject\n",
      "prepositional modifier\n",
      "object of preposition\n",
      "determiner\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"nsubj\"))\n",
    "print(spacy.explain(\"prep\"))\n",
    "print(spacy.explain(\"pobj\"))\n",
    "print(spacy.explain(\"det\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'nsubj'),\n",
       " ('enrolled', 'ROOT'),\n",
       " ('in', 'prep'),\n",
       " ('the', 'det'),\n",
       " ('course', 'pobj'),\n",
       " ('at', 'prep'),\n",
       " ('the', 'det'),\n",
       " ('university', 'pobj'),\n",
       " ('.', 'punct')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.text, t.dep_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we don't see the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'nsubj', 'enrolled'),\n",
       " ('enrolled', 'ROOT', 'enrolled'),\n",
       " ('in', 'prep', 'enrolled'),\n",
       " ('the', 'det', 'course'),\n",
       " ('course', 'pobj', 'in'),\n",
       " ('at', 'prep', 'course'),\n",
       " ('the', 'det', 'university'),\n",
       " ('university', 'pobj', 'at'),\n",
       " ('.', 'punct', 'enrolled')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To have a better idea, print the head of each dependency\n",
    "[(t.text, t.dep_, t.head.text) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word can be a child to only one head, while the same word can act as a head to zero, one, or multiple words.  \n",
    "\n",
    "The word \"enrolled\" has no arcs pointing to it. In this sentence it acts as the root. The finite verb is often the root of a sentence.  \n",
    "\n",
    "**One major advantage of dependency parsers: they are more resilient to word order changes.**  \n",
    "\n",
    "Exercice, compare the two following sentences:  \n",
    "- \"He looked at the paperwork **wearily**.\"  \n",
    "- \"He **wearily** looked at the paperwork.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Determining the syntactic structure of a sentence with Parsing can be useful to:</u>\n",
    "\n",
    "- **Grammar checking**: if a sentence cannot be parsed it may be grammatically incorrect or difficult to read.\n",
    "\n",
    "- **Question answering**: Parse structure can help resolving ambiguities by identifying likely dependencies between words, such as what the subjects and objects of a sentence are.  \n",
    "\n",
    "- **Sementic parsing**: e.g. convert natural language utterances to an intermediate representation interpretable by a machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing have a lot of applications:  \n",
    "\n",
    "- Automatic translation: understand the grammatical structure of a sentence in the source language before translate into the target language.\n",
    "\n",
    "- Sentiment analysis: use Parsing to better interpret the nuances and the context of the opinions expressed in the texts\n",
    "\n",
    "- The ChatBots (Siri, Alexia ...) use Parsing to understant the vocal commands and the users requests\n",
    "\n",
    "- Google...:  use Parsing to understand the users requests and give pertinent answers\n",
    "\n",
    "- Extract information: to extract structured information from unstructure information (names, places, dates...)  \n",
    "\n",
    "- Grammatical correction, Automatic summary, analyse financial texts or medical ones... to extract the pertinent information and facilitate the decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Exercices:**</u>\n",
    "\n",
    "**Parsing, combined with POS tagging and NER, can form the basis of an information extraction system**  \n",
    "\n",
    "Exemple of extraction: Financial corpus $\\Rightarrow$ Key entities and their relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Constituency Parsing vs Dependency Parsing**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constituency Parsing: to extract sub-phrases from a sentence  \n",
    "\n",
    "Dependency Parsing for the others applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9. **Matcher**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using spaCy's Matcher to find patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy comes with a host of pattern-matching functionality to help us to find patterns in a document.  \n",
    "\n",
    "Beyond regex, spaCy can match on a variety of attributes such as speech tags, POS tags, entity tags, lemmas, dependencies phrases...  \n",
    "\n",
    "https://spacy.io/usage/rule-based-matching\n",
    "https://explosion.ai/demos/matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Exercices:**</u>\n",
    "\n",
    "Here we try to search for patterns that may be useful for a hospitality bot.\n",
    "\n",
    "For that we use the basic matcher to find a verb noun combination in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The general Matcher is one of multiple matcher objects\n",
    "# included with spaCy.\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize the Matcher with the spaCy vocab object, which contains\n",
    "# words along with their labels and tags.\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"I want to book a hotel room.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterns are expressed as an ordered sequence. \n",
    "# Here, we're looking to match occurences starting with a 'book' string followed by \n",
    "# a determiner (DET) POS tag such as \"the\",\"and\" , then a noun POS tag.\n",
    "# The OP key marks the match as optional in some way.\n",
    "\n",
    "# Here, the DET POS (marked with '?') will match 0 or 1 times (i.e. the determiner is optional), and \n",
    "#¬†the NOUN POS (marked with '+') will match 1 or more times (i.e., at least one noun is required).\n",
    "# See this link for more information.\n",
    "# https://spacy.io/usage/rule-based-matching#quantifiers\n",
    "\n",
    "pattern = [\n",
    "    {'TEXT': 'book'},\n",
    "    {'POS': 'DET', 'OP': '?'},\n",
    "    {'POS': 'NOUN', 'OP': '+'}\n",
    "]\n",
    "\n",
    "# So, the pattern will match sequences that start with the word \"book\", \n",
    "# optionally followed by a determiner, and then followed by one or more nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches:  ['book a hotel', 'book a hotel room']\n"
     ]
    }
   ],
   "source": [
    "#¬†We give our pattern a label and pass it to the matcher.\n",
    "matcher.add('USER_INTENT', [pattern])\n",
    "\n",
    "# Run the matcher over the doc.\n",
    "matches = matcher(doc)\n",
    "\n",
    "# For each match, the matcher returns a tuple specifying a match id, start,\n",
    "# and end of the match.\n",
    "print(\"Matches: \", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above demonstrates the Matcher but is fragile.\n",
    "- What if \"book\" is capitalized?\n",
    "- What if a user types \"reserve\" instead of \"book\"?\n",
    "- How can we match on \"hotel room\" as a compound noun?\n",
    "- What if a user types \"book a flight and hotel room\"?\n",
    "- Can you think of how you would handle these cases?\n",
    "\n",
    "We could come up more rules to match different patterns,  \n",
    "or perhaps just search for keywords based on POS and entities (e.g. a country) and   \n",
    "present the user with a bunch of possible intentions and let them choose one,   \n",
    "or have a bunch of different interpretation functions submit answers and  \n",
    "select the most likely one based on what was historically accepted most often.  \n",
    "We can also ask clarifying questions to narrow things down.  \n",
    "\n",
    "For example, for the last sentence, you could have a function scan through the Doc object's noun_chunks  \n",
    "(phrases that have a noun as their head) and  \n",
    "isolate keywords there along with potential conjunctions (e.g. \"and\").  \n",
    "\n",
    "https://spacy.io/usage/linguistic-features#noun-chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we could look at noun chuncks instead and look at the phrase head for each chunck  \n",
    "which will provide more information.  \n",
    "So this extraction here is more illuminating in terms of what the person wants  \n",
    "to do and where:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase: I, root head: want\n",
      "phrase: a flight and hotel room, root head: book\n",
      "phrase: Berlin, root head: in\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I want to book a flight and hotel room in Berlin.\")\n",
    "for noun_phrase in doc.noun_chunks:\n",
    "  print(\"phrase: {}, root head: {}\".format(noun_phrase, noun_phrase.root.head))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The person wants something  \n",
    "There is a noun phrase containing nouns relevant to our domain  \n",
    "and the associated root is book and there's also a location along with a determiner.  \n",
    "\n",
    "For where we can further use named entity recognation to isolate Berlin as a destination and  \n",
    "disambiguate it from the other noun such as flight and hotel room.  \n",
    "\n",
    "Now there is still some ambiguities here, does the person wants this hotel room in Berlin or book from Berlin ?  \n",
    "In most cases though it's the former.  \n",
    "\n",
    "So you'll find that we can pile on rule after rule and we still not quiet get there!  \n",
    "\n",
    "Most of the systems today are a combination of layered approaches.  \n",
    "If we have a narrow domain with a finite set of cases such as ChatBots using these rules can help to make the job get done fast.  \n",
    "Because free form ChatBots are terrible and a bad ChatBot is infinitely worst than no ChatBot.  \n",
    "\n",
    "So for ChatBot you could start with a prototype but when the requirements become more sophisticated and the cases more varied  \n",
    "we'll need to blend in more powerful techniques.  \n",
    "\n",
    "https://spacy.io/usage/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talkin' like Yoda\n",
    "\n",
    "Languages like English are built around the subject-verb-object pattern. But if you're familiar with Yoda from Star Wars,   \n",
    "he famously speaks in an object-subject-verb pattern. Using the information in a dependency parse,   \n",
    "we can turn basic English sentences into Yoda-speak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yodize(s: str):\n",
    "  doc = nlp(s)\n",
    "  for t in doc:\n",
    "    if t.dep_ == \"ROOT\":\n",
    "\n",
    "      # Assuming our sentence is of the form subject-verb-object, we take \n",
    "      # everything after the root (likely verb) and put it in front, and \n",
    "      # likewise take everything before the root, and put it after.\n",
    "      seq = [doc[t.i + 1: -1].text, doc[0: t.i].text, t.text + '.']\n",
    "      seq[0] = seq[0].capitalize()\n",
    "      print(' '.join(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To texas I will fly.\n"
     ]
    }
   ],
   "source": [
    "yodize(\"I will fly to Texas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 0, 'PRON', 'nsubj')\n",
      "('will', 1, 'AUX', 'aux')\n",
      "('fly', 2, 'VERB', 'ROOT')\n",
      "('to', 3, 'ADP', 'prep')\n",
      "('Texas', 4, 'PROPN', 'pobj')\n",
      "('.', 5, 'PUNCT', 'punct')\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I will fly to Texas.\")\n",
    "for t in doc:\n",
    "    print((t.text, t.i, t.pos_, t.dep_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "will\n",
      "fly\n",
      "to\n",
      "Texas\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I will fly to Texas.\")\n",
    "for t in doc:\n",
    "    print(doc[t.i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will\n",
      "will fly\n",
      "fly to\n",
      "to Texas\n",
      "Texas.\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I will fly to Texas.\")\n",
    "for t in doc:\n",
    "    print(doc[t.i : t.i+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will\n",
      "will\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I will fly to Texas.\")\n",
    "for t in doc:\n",
    "    print(doc[t.i:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will fly to Texas\n",
      "fly to Texas\n",
      "to Texas\n",
      "Texas\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I will fly to Texas.\")\n",
    "for t in doc:\n",
    "    print(doc[t.i + 1: -1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10. Advanced Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: using doc.ents, identify and print the dates in this sentence.\n",
    "# Expected output: ['Feb 13th', 'Feb 24th']\n",
    "#\n",
    "s = \"We'll be in Osaka on Feb 13th and leave on Feb 24th.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Read about spaCy's PhraseMatcher\n",
    "# https://spacy.io/usage/rule-based-matching#phrasematcher\n",
    "#\n",
    "# Using the PhraseMatcher, find the start and end index of all occurrences \n",
    "# of 'Caesar Augustus' and 'Roman Empire' (case-insensitive).\n",
    "#\n",
    "# Expected output: [(0, 2), (15, 17)]\n",
    "#\n",
    "from spacy.matcher import PhraseMatcher\n",
    "s = \"Caesar Augustus was the founder of the Roman Principate (the first phase of the Roman Empire).\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Reading and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read through this page to learn more about spaCy's language processing pipeline including what's going on under the hood,  \n",
    "how to create custom components, disable certain components (e.g. NER)  \n",
    "when they're unneeded, optimization tips, and best practices:  \n",
    "https://spacy.io/usage/processing-pipelines\n",
    "\n",
    "Take the free and succinct spaCy course (available in multiple languages):  \n",
    "https://course.spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once we have our tokens and we've processed them to our liking, what's next ?**\n",
    "\n",
    "**We still have text but in order to use these tokens in statistical methods or machine learning algorithms**  \n",
    "\n",
    "**we need to transform them into numbers. And representing text as numbers is what we'll start exploring next.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
